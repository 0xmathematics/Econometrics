beta_ols = solve(t(Xtrain) %*% Xtrain, t(Xtrain) %*% Ytrain)
ols_in_samp_err = mean((Ytrain - Xtrain %*% beta_ols)^2)
ols_out_samp_err = mean((Ytest - Xtest %*% beta_ols)^2)
p = ncol(Xtrain)
lambda = 100  ## Try changing this
beta_ridge = solve(t(Xtrain) %*% Xtrain + lambda * diag(p), t(Xtrain) %*% Ytrain)
ridge_in_samp_err = mean((Ytrain - Xtrain %*% beta_ridge)^2)
ridge_out_samp_err = mean((Ytest - Xtest %*% beta_ridge)^2)
print(sprintf("OLS:  in-sample error is %.2f,  out-of-sample error is %.2f", ols_in_samp_err, ols_out_samp_err))
print(sprintf("Ridge:  in-sample error is %.2f,  out-of-sample error is %.2f", ridge_in_samp_err, ridge_out_samp_err))
set.seed(12)
cars = read.csv("cars.csv", as.is=TRUE)
Y = as.vector(cars[, "mpg"])
X = as.matrix(cars[, !(names(cars) %in% c("mpg", "name"))])
Y = scale(Y)
X = scale(X)
num_trash_vars = 1000  ## try changing this
X = cbind(X, matrix(rnorm(num_trash_vars * nrow(X)), nrow(X), num_trash_vars))
X = cbind(X, rep(1, nrow(X)))
train_ix = sample(nrow(X), floor(nrow(X)/8))
test_ix = (1:nrow(X))[-train_ix]
Xtrain = X[train_ix, ]
Xtest = X[test_ix, ]
Ytrain = Y[train_ix]
Ytest = Y[test_ix]
p = ncol(Xtrain)
lambda = 0.0001
beta_ridge = solve( t(Xtrain) %*% Xtrain + lambda*diag(p), t(Xtrain) %*% Ytrain)
plot(beta_ridge)
source("ISTA.R")
lambda = 0.0007  ## Try changing this
beta_lasso = lassoISTA(Xtrain, Ytrain, lambda)
plot(beta_lasso)
## NAME:Hang Miao
## NETID: hm363
####################################################
################
## Code for HW 1 problem 4
##
## INSTRUCTIONS:
## The following file performs refitted lasso regression with cross-validation
##
## Fill in the code in parts labeled "FILL IN".
##
################
source("ISTA.R")
cars = read.csv("cars.csv", as.is=TRUE)
set.seed(1)
Y = as.vector(cars[, "mpg"])
X = as.matrix(cars[, !(names(cars) %in% c("mpg", "name"))])
oldX = scale(X)
old_p = ncol(X)
## We create interaction features of the form
## column j * column j' for all (j, j')
## column j * (column j')^2 for all (j, j')
## (column j)^2 * (column j')^2 for all (j, j')
for (j in 1:old_p){
X = cbind(X, oldX*oldX[, j], oldX*oldX[, j]^2, oldX^2*oldX[, j], oldX^2 * oldX[,j]^2)
}
Y = scale(Y)
X = scale(X)
X = cbind(X, rep(1, nrow(X)))
n = 200
test_ix = sample(nrow(X), nrow(X) - n)
X1 = X[-test_ix, ]
X2 = X[test_ix, ]
Y1 = Y[-test_ix]
Y2 = Y[test_ix]
p = ncol(X1)
n = nrow(X1)
K = 10
## FILL IN: randomly permute the rows of X1
X1_ix <- sample(nrow(X1))
X1 <- X1[X1_ix,]
Y1 <- Y1[X1_ix]
lambda_ls = 10^(seq(-2, 1, 0.05))
errs = rep(0, length(lambda_ls))
length(errs)
# K=10, cross validation
for (k in 1:K){
valid_ix = ((k-1)*(n/K) + 1):(k*(n/K))
## FILL IN: create variables Xtrain, Ytrain, Xvalid, Yvalid
Xvalid <-X1[valid_ix,]
Xtrain <-X1[-valid_ix,]
Yvalid <-Y1[valid_ix]
Ytrain <-Y1[-valid_ix]
for (il in 1:length(lambda_ls)){
lambda = lambda_ls[il]
#beta_lasso = ## FILL IN: compute lasso estimate with ISTA
beta_lasso = lassoISTA(Xtrain, Ytrain, lambda)
S = which(abs(beta_lasso) > 1e-10)
if (length(S) == 0)
errs[il] = Inf
else {
XS = Xtrain[, S]
## For refitting, we use ridge regression with a small penalty instead of
## OLS in the event that the columns of X are not linearly independent
beta_refit = solve(t(XS) %*% XS + 1e-10 * diag(length(S)), t(XS) %*% Ytrain)
#errs[il] = ## FILL IN: compute error
errs[il] = errs[il] + mean((XS %*% beta_refit - Yvalid)^2)
}
}
}
#lambda_star = ## FILL IN: compute lambda_star
lambda_star = lambda_ls[which.min(errs)]
beta_lasso = lassoISTA(X1, Y1, lambda_star)
S = which(abs(beta_lasso) > 1e-10)
#beta_refit = ## FILL IN: compute the refitting on X1, Y1
XS = X1[, S]
beta_refit =  solve(t(XS) %*% XS + 1e-10 * diag(length(S)), t(XS) %*% Y1)
#test_error = ## FILL IN: compute the test error
test_error = mean((X2[, S] %*% beta_refit - Y2)^2)
## For comparison, we also compute the OLS
beta_ols = solve(t(X1) %*% X1 + 1e-10 * diag(ncol(X1)), t(X1) %*% Y1)
ols_error = mean((X2 %*% beta_ols - Y2)^2)
## We compute OLS where we only use the first 7 variables and
## the all 1 constant feature.
S = c(1:7, ncol(X1))
beta_ols2 = solve(t(X1[, S]) %*% X1[, S] + 1e-10 * diag(length(S)), t(X1[, S]) %*% Y1)
ols2_error = mean((X2[, S] %*% beta_ols2 - Y2)^2)
baseline = mean((mean(Y1) - Y2)^2)
print(sprintf("Test error: %.3f  Baseline: %.3f   OLS: %.3f   OLS (with first 7 vars): %.3f",
test_error, baseline, ols_error, ols2_error))
35.50+46.60 +94.00+++
35.50+46.60 +94.00
35.50+46.60 +94.00
75.00+78.20
# creat matrix
M1 = matrix(1:10, nrow = 5, ncol = 2, byrow = TRUE)
M1
M2 = matrix(rep(1,times = 10), nrow = 5, ncol = 2, byrow = TRUE)
M2
V1 = c(1:5)
V1
V2 = rep(1, times = 5)
V2
# vector  element wise product
V1 * V2
t(V1) %*% V2
M1
V1
M1 * V1
M5 = diag(c(1,2,3,4,5))
M5
### creat an identity matrix diagnol matrix with 5 by 5 dimension
M6 = diag(5)
M6
M5inv = solve(M5)
M5inv
M6inv = solve(M6)
################################
# Normal Underlying Population
################################
# The underlying population follows Normal(mu, sigma^2)
N = 10000000
mu = 20
sigma = 5
# randomly generate N samples from the underlying population
X = rnorm(N, mean = mu, sd = sigma)
E = mu
Var = sigma^2
sd = sigma
# Plot the histogram of N samples randomly draw
# from the underlying population distribution Normal(mu, sigma^2)
hist(X,
col = "steelblue" ,
freq = FALSE,
breaks = 50,
main = 'Sample Histogram and Underlying Distribution',cex.main=0.75)
# draw the the underlying population distribution Normal(mu, sigma^2)
x = seq(range(X)[1], range(X)[2], by = diff(range(X))/50 )
curve(dnorm(x, mean = mu, sd = sigma),
col = "red",
lwd = "2",
add = TRUE)
legend("topleft",c("Sample Histogram","Underlying Distribution"),fill=c("steelblue","red"),cex = 0.5 )
### small Sample and Large Sample
Sample_Size = c(10, 100000 )
### number of repetition
reps = 2000
###
#i=1 #small sample
#i=2 #large sample
i=1
## Conduct reps (2000) times of following experiment:
## each time draw sample_Size  many of samples from underlying population
## samples is a matrix with dimension Sample_Size*reps
samples <- replicate(reps, sample(X,size = Sample_Size[i],replace = FALSE, prob = NULL) )
## calculate the sample mean for each iteration
sample_Means <- colMeans(samples)
sample_Means
sample_Means
sample_Means[1:10]
length(sample_Means)
sample_Means[1990:2000]
library(AER)
data(CASchools) # Use data
CASchools$size <- CASchools$students/CASchools$teachers    # generating variable "size"
CASchools$score <- (CASchools$read + CASchools$math) / 2   # generating variable "score"
install.packages("AER")
library(AER)
data(CASchools) # Use data
CASchools$size <- CASchools$students/CASchools$teachers    # generating variable "size"
CASchools$score <- (CASchools$read + CASchools$math) / 2   # generating variable "score"
library(AER)
install.packages("AER")
library(AER)
install.packages("car")
install.packages("AER")
library(AER)
install.packages("car")
install.packages("AER")
data(CASchools) # Use data
CASchools$size <- CASchools$students/CASchools$teachers    # generating variable "size"
CASchools$score <- (CASchools$read + CASchools$math) / 2   # generating variable "score"
library(AER)
library(AER)
library(car)
library(car)
install.packages("data.table")
library(car)
library(data.table)
install.packages(c("boot", "foreign", "KernSmooth", "Matrix", "mgcv", "nlme", "survival"))
library(AER)
install.packages("car")
library(car)
library('car')
install.packages("car", dependencies=TRUE)
library('car')
library(car)
install.packages("pbkrtest")
library(car)
install.packages("car", dependencies=TRUE)
library(AER)
252/12
2^2
py0 = 0.0386+0.0549+0.1018
py1 = 0.1322+0.0202+0.1558
py2 = 0.1768+0.1352+0.1845
#7
E_Y = 0*py0+1*py1+2*py2
E_Square_Y = 0*py0+1^2*py1+2^2*py2
Var_Y = E_Square_Y - E_Y^2
round(Var_Y,digits = 4)
px1 = 0.0386+0.1322+0.1768
px2 = 0.0549+0.0202+0.1352
px3 = 0.1018+0.1558+0.1845
P_X1_con_Y0 = 0.0386/py0
P_X2_con_Y0 = 0.0549/py0
P_X3_con_Y0 = 0.1018/py0
E_X_con_Y0 = 1*P_X1_con_Y0 + 2*P_X2_con_Y0 + 3*P_X3_con_Y0
round(E_X_con_Y0,digits = 4)
0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352
+6*0.1845
px1 = 0.0386+0.1322+0.1768
px2 = 0.0549+0.0202+0.1352
px3 = 0.1018+0.1558+0.1845
E_X = 1*px1+2*px2+3*px3
E_Y = 0*py0+1*py1+2*py2
E_Square_Y = 0*py0+1^2*py1+2^2*py2
Var_Y = E_Square_Y - E_Y^2
round(Var_Y,digits = 4)
E_XY = 0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352
+6*0.1845
cov_XY = E_XY - E_X*E_Y
round(cov_XY,digits = 4)
E_Xsquare = 1^2 *px1 + 2^2*px2 + 3^2*px3
Var_X = E_Xsquare-E_X^2
round(Var_X, digits = 4)
E_squared_X = 1^2 *px1 + 2^2*px2 + 3^2*px3
Var_X = E_squared_X-E_X^2
round(Var_X, digits = 4)
corr_XY = cov_XY/sqrt(Var_X*Var_Y  )
round(corr_XY,digits = 4)
corr_XY = cov_XY/sqrt(Var_X*Var_Y  )
round(corr_XY,digits = 4)
E_Y = 0*py0+1*py1+2*py2
E_Square_Y = 0*py0+1^2*py1+2^2*py2
Var_Y = E_Square_Y - E_Y^2
round(Var_Y,digits = 4)
corr_XY = cov_XY/sqrt( Var_X*Var_Y )
round(corr_XY,digits = 4)
E_Y
E_Y^2
0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352
+6*0.1845
E_Y = 0*py0+1*py1+2*py2
E_Square_Y = 0*py0+1^2*py1+2^2*py2
Var_Y = E_Square_Y - E_Y^2
round(Var_Y,digits = 4)
E_XY = 0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352
+6*0.1845
E_XY
0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352
+6*0.1845
0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352+6*0.1845
E_XY = 0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352+6*0.1845
cov_XY = E_XY - E_X*E_Y
round(cov_XY,digits = 4)
E_Y
Var_Y
round(Var_Y,digits = 4)
corr_XY = cov_XY/sqrt( Var_X*Var_Y )
round(corr_XY,digits = 4)
E_XY = 0*(0.0386+0.0549+0.1018) + 1*0.1322 + 2*(0.1768+0.0202) + 3*0.1558 + 4*0.1352+6*0.1845
cov_XY = E_XY - E_X*E_Y
round(cov_XY,digits = 4)
corr_XY = cov_XY/sqrt( Var_X*Var_Y )
round(corr_XY,digits = 4)
corr_XY = cov_XY/sqrt( Var_X*Var_Y )
round(corr_XY,digits = 4)
dtibble <- read_excel("cps08.xlsx", col_types = "numeric")
df = data.frame(dtibble)
head(df)
setwd("~/Dropbox/Econometrics/R Emprical Analysis/class demo/Demo4 Wald Test and F Test")
## load excel file
dtibble <- read_excel("cps08.xlsx", col_types = "numeric")
df = data.frame(dtibble)
head(df)
############################################################################
library('readxl')
dtibble <- read_excel("cps08.xlsx", col_types = "numeric")
df = data.frame(dtibble)
head(df)
head(df)
### matrix method
y = df$ahe
X = df[ , c(3,4,5)]
n = nrow(df)
intercept = rep(1,nrow(df))
X = cbind(intercept,X)
X = as.matrix(X)
y = as.matrix(y)
betaOLS = solve( t(X) %*% X ) %*% t(X) %*% y
SSR = t(y- X %*%betaOLS )%*% (y- X %*%betaOLS )
sigmasqaureHat = SSR/ (n-3-1)
sigmasqaureHat
sigmasqaureHat = as.numeric(sigmasqaureHat)
sigmasqaureHat
VarCovMatrixBetaOLS = sigmasqaureHat * solve(t(X) %*% X)
VarCovMatrixBetaOLS
VarBetaOLS = diag(VarCovMatrixBetaOLS)
sdBetaOLS = sqrt(VarBetaOLS)
VarCovMatrixBetaOLS
linearMod2 <- lm(ahe ~ female+bachelor+age, data=df)
summary(linearMod2)
summary(linearMod2)$coef
df
linearMod2 <- lm(ahe ~ female+bachelor+age, data=df)
linearMod2
linearMod2
summary(linearMod2)
A = cbind(rep(0,3) ,diag(3))
A
a = as.vector(rep(0,3))
a
d
d = 3
d
Wald_Stat = t(A %*% betaOLS-a) %*% solve(sigmasqaureHat*A %*% solve(t(X) %*% X) %*% t(A) ) %*% (A %*% betaOLS-a)
F_Stat = Wald_Stat/d
critical_Value = qchisq(Sig_alpha, df)
Sig_alpha = 0.9
df = d
# Critical Value
critical_Value = qchisq(Sig_alpha, df)
critical_Value
Wald_Stat
Sig_alpha = 0.9
df1 = d
df2 = n-3-1
# Critical Value
critical_Value = qf(Sig_alpha, df1,df2)
critical_Value
# P-Value
pvalue = pf(F_Stat, df1,df2, lower.tail = FALSE)
F_Stat
summary(linearMod2)
setwd("~/Dropbox/Econometrics/R Emprical Analysis/exams/untitled folder")
X = rnorm(1000, 15, 30)
Y = rnorm(2000, 10, 40)
X = data.frame(X)
Y = data.frame(Y)
head(X)
head(Y)
tail(X)
tail(Y)
X <- read.csv("Dataset24_35_X.csv")
Y <- read.csv("Dataset24_35_Y.csv")
nrow(X)
nrow(Y)
x = X[,1]
y = Y[,1]
n = length(x)
n
m = length(y)
m
x_bar = mean(x)
y_bar = mean(y)
x_bar-y_bar
#variance of the estimator
var_mean_Diff = var_x/n+var_y/m
var_mean_Diff
#sd of the estimator
sd_mean_Diff = sqrt(var_mean_Diff)
sd_mean_Diff
#variance of the x
var_x = 30
var_x
#variance of the y
var_y = 40
var_mean_Diff = var_x/n+var_y/m
var_mean_Diff
sd_mean_Diff = sqrt(var_mean_Diff)
sd_mean_Diff
# statistics
z = (x_bar - y_bar )/sd_mean_Diff
z
qnorm(0.95)
# P-value
pnorm(z)
pnorm(z)
cv = qnorm(0.975)
left = x_bar - y_bar - cv*sd_mean_Diff
right = x_bar - y_bar + cv*sd_mean_Diff
CI95 = c(left,right)
round(CI95, digits = 3)
df <- read.csv("Dataset11_23.csv")
x = df[,1]
n = length(x)
x_bar = mean(x)
#variance of the X
var_x = sum((x - x_bar)^2)/(n-1)
var_x
var_x = sum((x - x_bar)^2)/n
var_x = sum((x - x_bar)^2)/n
var_x
df <- read.csv("Dataset11_23.csv")
x = df[,1]
n = length(x)
##############################
## Analysis
##############################
x_bar = mean(x)
#variance of the X
var_x = sum((x - x_bar)^2)/(n-1)
var_x
var_x = sum((x - x_bar)^2)/n
var_x
var_xbar = var_x/n
var_xbar
sd_xbar = sqrt(var_xbar)
sd_xbar
z = (x_bar - 22)/sd_xbar
z
qnorm(0.10)
sd_xbar
var_xbar = var_x/n
var_xbar
n
z
z = (x_bar - 22)/sd_xbar
z
z = (x_bar - 22)/0.01
z
var_xbar
z = (x_bar - 22)/sqrt(0.01)
z
z = (x_bar - 22)/sd_xbar
z
qt(.90)
df <- read.csv("Dataset49_62.csv")
head(df)
n = nrow(df)
y = df[, 1]
X = df[ , c(2,3,4,5)]
n = nrow(df)
intercept = rep(1,nrow(df))
X = cbind(intercept,X)
qt(.90,df=n-4-1)
qt(.90,df=n-4-1)
1-pt(tstat, 35)
tstat = (2.734754-2)/0.2564438
1-pt(tstat, 35)
1-pt(tstat, 35)
left = 2.865166  - cv*0.2564438
right = 2.865166  + cv*0.2564438
CI90 = c(left,right)
round(CI90, digits = 3)
CI90 = c(left,right)
CI90
df <- read.csv("Dataset36_48.csv")
head(df)
y = df[, 1]
X = df[ , c(2,3,4,5)]
n = nrow(df)
intercept = rep(1,nrow(df))
X = cbind(intercept,X)
X = as.matrix(X)
y = as.matrix(y)
betaOLS = solve( t(X) %*% X ) %*% t(X) %*% y
SSR = t(y- X %*%betaOLS )%*% (y- X %*%betaOLS )
sigmasqaureHat = SSR/ (n-4-1)
sigmasqaureHat = as.numeric(sigmasqaureHat)
VarCovMatrixBetaOLS = sigmasqaureHat * solve(t(X) %*% X)
VarBetaOLS = diag(VarCovMatrixBetaOLS)
sdBetaOLS = sqrt(VarBetaOLS)
round(sdBetaOLS, digits = 3)
betaOLS
sdBetaOLS
-1.7269658/0.2816299
-1.7269658/0.2816299
